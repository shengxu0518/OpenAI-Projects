{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from reinforce_lib import func_approx_library as funclib\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "class QLearner():\n",
    "    # load in simulator, initialize global variables\n",
    "    def __init__(self,simulator,savename,**kwargs):\n",
    "        # make simulator global\n",
    "        self.simulator = simulator\n",
    "        \n",
    "        # Q learn params\n",
    "        self.explore_val = 1\n",
    "        self.explore_decay = 0.99\n",
    "        self.num_episodes = 500        \n",
    "        self.gamma = 1\n",
    "        \n",
    "        if \"gamma\" in kwargs:   \n",
    "            self.gamma = args['gamma']\n",
    "        if 'explore_val' in kwargs:\n",
    "            self.explore_val = kwargs['explore_val']\n",
    "        if 'explore_decay' in kwargs:\n",
    "            self.explore_decay = kwargs['explore_decay']\n",
    "        if 'num_episodes' in kwargs:\n",
    "            self.num_episodes = kwargs['num_episodes']\n",
    "            \n",
    "        # other training variables\n",
    "        self.num_actions = self.simulator.action_space.n\n",
    "        state = self.simulator.reset()    \n",
    "        self.state_dim = np.size(state)\n",
    "        self.training_reward = []\n",
    "        \n",
    "        # setup memory params\n",
    "        self.memory_length = 2000    # length of memory replay\n",
    "        self.replay_length = 200     # length of replay sample\n",
    "        self.memory_start = 1000\n",
    "        self.memory = []\n",
    "        if 'memory_length' in kwargs:\n",
    "            self.memory_length = kwargs['memory_length']\n",
    "        if 'replay_length' in kwargs:\n",
    "            self.replay_length = kwargs['replay_length']\n",
    "        if 'memory_start' in kwargs:\n",
    "            self.memory_start = kwargs['memory_start']\n",
    "            \n",
    "        ### initialize logs ###\n",
    "        # create text file for training log\n",
    "        self.logname = 'training_logs/' + savename + '.txt'\n",
    "        self.reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "        self.weight_name = 'saved_model_weights/' + savename + '.pkl' \n",
    "        self.model_name = 'models/' + savename + '.json'\n",
    "\n",
    "        self.init_log(self.logname)\n",
    "        self.init_log(self.reward_logname)\n",
    "        self.init_log(self.weight_name)\n",
    "        self.init_log(self.model_name)\n",
    "     \n",
    "    ##### logging functions #####\n",
    "    def init_log(self,logname):\n",
    "        # delete log if old version exists\n",
    "        if os.path.exists(logname): \n",
    "            os.remove(logname)\n",
    "            \n",
    "    def update_log(self,logname,update):\n",
    "        if type(update) == str:\n",
    "            logfile = open(logname, \"a\")\n",
    "            logfile.write(update)\n",
    "            logfile.close() \n",
    "        else:\n",
    "            weights = []\n",
    "            if os.path.exists(logname):\n",
    "                with open(logname,'rb') as rfp: \n",
    "                    weights = pickle.load(rfp)\n",
    "            weights.append(update)\n",
    "\n",
    "            with open(logname,'wb') as wfp:\n",
    "                pickle.dump(weights, wfp)\n",
    "    \n",
    "    ##### functions for creating / updating Q #####\n",
    "    def initialize_Q(self,**kwargs):\n",
    "        # default parameters for network\n",
    "        layer_sizes = [10,10]      # two hidden layers, 10 units each, by default\n",
    "        activation = 'relu'\n",
    "        if 'layer_sizes' in kwargs:\n",
    "            layer_sizes = kwargs['layer_sizes']\n",
    "        if 'activation' in kwargs:\n",
    "            activation = kwargs['activation']\n",
    "\n",
    "        # default parameters for optimizer - reset by hand\n",
    "        loss = 'mse'\n",
    "        self.lr = 10**(-2)\n",
    "        if 'alpha' in kwargs:\n",
    "            self.lr = kwargs['alpha']\n",
    "\n",
    "        # input / output sizes of network\n",
    "        input_dim = self.state_dim\n",
    "        output_dim = self.num_actions\n",
    "    \n",
    "        # setup network\n",
    "        layer_sizes.insert(0,input_dim)\n",
    "        layer_sizes.append(output_dim)\n",
    "\n",
    "        # setup architecture, choose cost, and setup architecture\n",
    "        self.model = funclib.super_setup.Setup()\n",
    "        self.model.choose_cost(name = 'least_squares')\n",
    "        self.model.choose_features(layer_sizes = layer_sizes,activation = activation)\n",
    "            \n",
    "        # initialize Q\n",
    "        self.Q = self.model.predict\n",
    "\n",
    "    # update Q function\n",
    "    def update_Q(self,state,next_state,action,reward,done):\n",
    "        # add newest sample to queue\n",
    "        self.update_memory(state,next_state,action,reward,done)\n",
    "        \n",
    "        # only update Q if sufficient memory has been collected\n",
    "        if len(self.memory) < self.memory_start:\n",
    "            return        \n",
    "        \n",
    "        # update memory sample\n",
    "        self.sample_memory()\n",
    "        \n",
    "        # generate q_values based on most recent Q\n",
    "        q_vals = []\n",
    "        states = []\n",
    "        for i in range(len(self.replay_samples)):    \n",
    "            # get sample\n",
    "            sample = self.replay_samples[i]\n",
    "            \n",
    "            # strip sample for parts\n",
    "            state = sample[0]\n",
    "            next_state = sample[1]\n",
    "            action = sample[2]\n",
    "            reward = sample[3]\n",
    "            done = sample[4]\n",
    "                            \n",
    "            ### for cartpole only - check if done, and alter reward to improve learning ###\n",
    "            #done,reward = self.check_done(done,reward)\n",
    "\n",
    "            # compute and store q value\n",
    "            q = reward \n",
    "            if done == False:\n",
    "                qs = self.Q(next_state.T)\n",
    "                q += self.gamma*np.max(qs)\n",
    "            \n",
    "            # clamp all other models to their current values for this input/output pair\n",
    "            q_update = self.Q(state.T).flatten()\n",
    "            q_update[action] = q\n",
    "            q_vals.append(q_update)\n",
    "            states.append(state.T)\n",
    "            \n",
    "        # convert lists to numpy arrays for regressor\n",
    "        s_in = np.array(states).T\n",
    "        q_vals = np.array(q_vals).T\n",
    "        s_in = s_in[0,:,:]\n",
    "                            \n",
    "        # take descent step\n",
    "        self.model.fit(s_in,q_vals,algo = 'RMSprop',max_its = 1,alpha = self.lr,verbose = False)\n",
    "        \n",
    "        # update Q based on regressor updates\n",
    "        self.Q = self.model.predict\n",
    "        \n",
    "    ##### functions for adjusting replay memory #####\n",
    "    # update memory - add sample to list, remove oldest samples \n",
    "    def update_memory(self,state,next_state,action,reward,done):\n",
    "        # add most recent trial data to memory\n",
    "        self.memory.append([state,next_state,action,reward,done])\n",
    "\n",
    "        # clip memory if it gets too long    \n",
    "        num_elements = len(self.memory)\n",
    "        if num_elements >= self.memory_length:    \n",
    "            num_delete = num_elements - self.memory_length\n",
    "            self.memory[:num_delete] = []\n",
    "    \n",
    "    # sample from memory and create input / output pairs for regression\n",
    "    def sample_memory(self):\n",
    "        # indices to sample\n",
    "        memory_num = len(self.memory)\n",
    "        sample_nums = np.random.permutation(memory_num)[:self.replay_length]\n",
    "\n",
    "        # create samples\n",
    "        self.replay_samples = [self.memory[v] for v in sample_nums]\n",
    "    \n",
    "    ##### Q Learning functionality #####\n",
    "    # state normalizer\n",
    "    def state_normalizer(self,states):\n",
    "        states = np.array(states)[np.newaxis,:]\n",
    "        return states\n",
    "    \n",
    "    # choose next action\n",
    "    def choose_action(self,state):\n",
    "        # pick action at random\n",
    "        p = np.random.rand(1)   \n",
    "        action = np.random.randint(self.num_actions)\n",
    "            \n",
    "        # pick action based on exploiting\n",
    "        if len(self.memory) >= self.memory_start:\n",
    "            qs = self.Q(state.T) \n",
    "            if p > self.explore_val:\n",
    "                action = np.argmax(qs)\n",
    "        return action\n",
    "\n",
    "    # special function to check done\n",
    "    def check_done(self,done,reward):\n",
    "        if done == True:\n",
    "            reward = -100\n",
    "        return done,reward\n",
    "    \n",
    "    # main training function\n",
    "    def train(self,**kwargs):        \n",
    "        ### start main Q-learning loop ###\n",
    "        for n in range(self.num_episodes): \n",
    "            # pick this episode's starting position - randomly initialize from f_system\n",
    "            state = self.simulator.reset()    \n",
    "            state = self.state_normalizer(state)\n",
    "            total_episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            # get out exploit parameter for this episode\n",
    "            if self.explore_val > 0.01:\n",
    "                self.explore_val *= self.explore_decay\n",
    "                    \n",
    "            # run episode\n",
    "            step = 0\n",
    "            while done == False:    \n",
    "                # choose next action\n",
    "                action = self.choose_action(state)\n",
    "    \n",
    "                # transition to next state, get associated reward\n",
    "                next_state,reward,done,info = self.simulator.step(action)  \n",
    "                next_state = self.state_normalizer(next_state)\n",
    "                \n",
    "                # update Q function\n",
    "                self.update_Q(state,next_state,action,reward,done)  \n",
    "\n",
    "                # update total reward from this episode\n",
    "                total_episode_reward+=reward\n",
    "                state = copy.deepcopy(next_state)\n",
    "                  \n",
    "            # start storing once memory is full \n",
    "            if len(self.memory) >= self.memory_start:\n",
    "                # print out update if verbose set to True\n",
    "                update = 'training episode ' + str(n+1) +  ' of ' + str(self.num_episodes) + ' complete, ' +  ' explore val = ' + str(np.round(self.explore_val,3)) + ', episode reward = ' + str(np.round(total_episode_reward,2)) \n",
    "\n",
    "                self.update_log(self.logname,update + '\\n')\n",
    "                print (update)\n",
    "\n",
    "                update = str(total_episode_reward) + '\\n'\n",
    "                self.update_log(self.reward_logname,update)\n",
    "\n",
    "                ### store this episode's computation time and training reward history\n",
    "                self.training_reward.append(total_episode_reward)\n",
    "\n",
    "                # save latest weights from this episode \n",
    "                update = self.model.weight_history[-1]\n",
    "                self.update_log(self.weight_name,update)\n",
    "            \n",
    "        ### save weights ###\n",
    "        update = 'q-learning algorithm complete'\n",
    "        self.update_log(self.logname,update + '\\n')\n",
    "        print (update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import gym\n",
    "\n",
    "# savename\n",
    "savename = 'lunarlander_experiment_1'\n",
    "\n",
    "# initialize simulator\n",
    "simulator = gym.make('LunarLander-v2') \n",
    "\n",
    "# initialize Q Learn process\n",
    "num_episodes = 10000\n",
    "explore_decay = 1\n",
    "explore_val = 0.01\n",
    "\n",
    "# initialize memory\n",
    "replay_length = 100\n",
    "memory_length = 1000\n",
    "\n",
    "# load into instance of learner\n",
    "demo = QLearner(simulator,savename,num_episodes=num_episodes,explore_decay=explore_decay,explore_val=explore_val,memory_length=memory_length,replay_length=replay_length)\n",
    "\n",
    "# initialize Q function\n",
    "layer_sizes = [25,25]\n",
    "alpha = 10**(-2)\n",
    "activation = 'tanh'\n",
    "demo.initialize_Q(layer_sizes=layer_sizes,alpha=alpha,activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training episode 12 of 2000 complete,  explore val = 0.01, episode reward = -178.19\n",
      "training episode 13 of 2000 complete,  explore val = 0.01, episode reward = -157.78\n",
      "training episode 14 of 2000 complete,  explore val = 0.01, episode reward = -340.74\n",
      "training episode 15 of 2000 complete,  explore val = 0.01, episode reward = -512.05\n",
      "training episode 16 of 2000 complete,  explore val = 0.01, episode reward = -509.16\n",
      "training episode 17 of 2000 complete,  explore val = 0.01, episode reward = -593.72\n",
      "training episode 18 of 2000 complete,  explore val = 0.01, episode reward = -199.03\n",
      "training episode 19 of 2000 complete,  explore val = 0.01, episode reward = -188.02\n",
      "training episode 20 of 2000 complete,  explore val = 0.01, episode reward = -221.08\n",
      "training episode 21 of 2000 complete,  explore val = 0.01, episode reward = -324.95\n",
      "training episode 22 of 2000 complete,  explore val = 0.01, episode reward = -93.61\n",
      "training episode 23 of 2000 complete,  explore val = 0.01, episode reward = -429.61\n",
      "training episode 24 of 2000 complete,  explore val = 0.01, episode reward = -448.75\n",
      "training episode 25 of 2000 complete,  explore val = 0.01, episode reward = -437.22\n",
      "training episode 26 of 2000 complete,  explore val = 0.01, episode reward = -139.89\n",
      "training episode 27 of 2000 complete,  explore val = 0.01, episode reward = -711.4\n",
      "training episode 28 of 2000 complete,  explore val = 0.01, episode reward = -301.92\n",
      "training episode 29 of 2000 complete,  explore val = 0.01, episode reward = -191.18\n",
      "training episode 30 of 2000 complete,  explore val = 0.01, episode reward = -391.75\n",
      "training episode 31 of 2000 complete,  explore val = 0.01, episode reward = -1519.69\n",
      "training episode 32 of 2000 complete,  explore val = 0.01, episode reward = -182.66\n",
      "training episode 33 of 2000 complete,  explore val = 0.01, episode reward = -329.61\n",
      "training episode 34 of 2000 complete,  explore val = 0.01, episode reward = -61.05\n",
      "training episode 35 of 2000 complete,  explore val = 0.01, episode reward = -22.28\n",
      "training episode 36 of 2000 complete,  explore val = 0.01, episode reward = -186.76\n",
      "training episode 37 of 2000 complete,  explore val = 0.01, episode reward = -168.26\n",
      "training episode 38 of 2000 complete,  explore val = 0.01, episode reward = -154.64\n",
      "training episode 39 of 2000 complete,  explore val = 0.01, episode reward = -167.69\n",
      "training episode 40 of 2000 complete,  explore val = 0.01, episode reward = -165.95\n",
      "training episode 41 of 2000 complete,  explore val = 0.01, episode reward = -197.55\n",
      "training episode 42 of 2000 complete,  explore val = 0.01, episode reward = -136.31\n",
      "training episode 43 of 2000 complete,  explore val = 0.01, episode reward = -177.12\n",
      "training episode 44 of 2000 complete,  explore val = 0.01, episode reward = -192.31\n",
      "training episode 45 of 2000 complete,  explore val = 0.01, episode reward = -151.35\n",
      "training episode 46 of 2000 complete,  explore val = 0.01, episode reward = -191.21\n",
      "training episode 47 of 2000 complete,  explore val = 0.01, episode reward = -127.12\n",
      "training episode 48 of 2000 complete,  explore val = 0.01, episode reward = -162.87\n",
      "training episode 49 of 2000 complete,  explore val = 0.01, episode reward = -177.42\n",
      "training episode 50 of 2000 complete,  explore val = 0.01, episode reward = -189.35\n",
      "training episode 51 of 2000 complete,  explore val = 0.01, episode reward = -203.4\n",
      "training episode 52 of 2000 complete,  explore val = 0.01, episode reward = -198.5\n",
      "training episode 53 of 2000 complete,  explore val = 0.01, episode reward = -209.99\n",
      "training episode 54 of 2000 complete,  explore val = 0.01, episode reward = -159.84\n",
      "training episode 55 of 2000 complete,  explore val = 0.01, episode reward = -340.14\n",
      "training episode 56 of 2000 complete,  explore val = 0.01, episode reward = -307.72\n",
      "training episode 57 of 2000 complete,  explore val = 0.01, episode reward = -342.33\n",
      "training episode 58 of 2000 complete,  explore val = 0.01, episode reward = -159.93\n",
      "training episode 59 of 2000 complete,  explore val = 0.01, episode reward = -467.4\n",
      "training episode 60 of 2000 complete,  explore val = 0.01, episode reward = -583.39\n",
      "training episode 61 of 2000 complete,  explore val = 0.01, episode reward = -346.1\n",
      "training episode 62 of 2000 complete,  explore val = 0.01, episode reward = -277.6\n",
      "training episode 63 of 2000 complete,  explore val = 0.01, episode reward = -164.39\n",
      "training episode 64 of 2000 complete,  explore val = 0.01, episode reward = -144.05\n",
      "training episode 65 of 2000 complete,  explore val = 0.01, episode reward = -91.21\n",
      "training episode 66 of 2000 complete,  explore val = 0.01, episode reward = -104.96\n",
      "training episode 67 of 2000 complete,  explore val = 0.01, episode reward = -162.22\n",
      "training episode 68 of 2000 complete,  explore val = 0.01, episode reward = -179.36\n",
      "training episode 69 of 2000 complete,  explore val = 0.01, episode reward = -335.7\n",
      "training episode 70 of 2000 complete,  explore val = 0.01, episode reward = -138.46\n",
      "training episode 71 of 2000 complete,  explore val = 0.01, episode reward = -277.27\n",
      "training episode 72 of 2000 complete,  explore val = 0.01, episode reward = -157.67\n",
      "training episode 73 of 2000 complete,  explore val = 0.01, episode reward = -159.99\n",
      "training episode 74 of 2000 complete,  explore val = 0.01, episode reward = -82.04\n",
      "training episode 75 of 2000 complete,  explore val = 0.01, episode reward = -978.85\n",
      "training episode 76 of 2000 complete,  explore val = 0.01, episode reward = -275.25\n",
      "training episode 77 of 2000 complete,  explore val = 0.01, episode reward = -159.51\n",
      "training episode 78 of 2000 complete,  explore val = 0.01, episode reward = -223.02\n",
      "training episode 79 of 2000 complete,  explore val = 0.01, episode reward = -429.3\n",
      "training episode 80 of 2000 complete,  explore val = 0.01, episode reward = -212.32\n",
      "training episode 81 of 2000 complete,  explore val = 0.01, episode reward = -140.5\n",
      "training episode 82 of 2000 complete,  explore val = 0.01, episode reward = -408.79\n",
      "training episode 83 of 2000 complete,  explore val = 0.01, episode reward = -218.23\n",
      "training episode 84 of 2000 complete,  explore val = 0.01, episode reward = -279.71\n",
      "training episode 85 of 2000 complete,  explore val = 0.01, episode reward = -167.77\n",
      "training episode 86 of 2000 complete,  explore val = 0.01, episode reward = -191.16\n",
      "training episode 87 of 2000 complete,  explore val = 0.01, episode reward = -272.56\n",
      "training episode 88 of 2000 complete,  explore val = 0.01, episode reward = -181.49\n",
      "training episode 89 of 2000 complete,  explore val = 0.01, episode reward = -215.34\n",
      "training episode 90 of 2000 complete,  explore val = 0.01, episode reward = -161.19\n",
      "training episode 91 of 2000 complete,  explore val = 0.01, episode reward = -144.42\n",
      "training episode 92 of 2000 complete,  explore val = 0.01, episode reward = -123.18\n",
      "training episode 93 of 2000 complete,  explore val = 0.01, episode reward = -749.85\n",
      "training episode 94 of 2000 complete,  explore val = 0.01, episode reward = -202.31\n",
      "training episode 95 of 2000 complete,  explore val = 0.01, episode reward = -135.35\n",
      "training episode 96 of 2000 complete,  explore val = 0.01, episode reward = -383.12\n",
      "training episode 97 of 2000 complete,  explore val = 0.01, episode reward = -67.08\n",
      "training episode 98 of 2000 complete,  explore val = 0.01, episode reward = -192.79\n",
      "training episode 99 of 2000 complete,  explore val = 0.01, episode reward = -297.01\n",
      "training episode 100 of 2000 complete,  explore val = 0.01, episode reward = -293.86\n",
      "training episode 101 of 2000 complete,  explore val = 0.01, episode reward = -156.0\n",
      "training episode 102 of 2000 complete,  explore val = 0.01, episode reward = -795.5\n",
      "training episode 103 of 2000 complete,  explore val = 0.01, episode reward = -289.55\n",
      "training episode 104 of 2000 complete,  explore val = 0.01, episode reward = -78.4\n",
      "training episode 105 of 2000 complete,  explore val = 0.01, episode reward = -146.11\n",
      "training episode 106 of 2000 complete,  explore val = 0.01, episode reward = -404.68\n",
      "training episode 107 of 2000 complete,  explore val = 0.01, episode reward = -289.75\n",
      "training episode 108 of 2000 complete,  explore val = 0.01, episode reward = -429.81\n",
      "training episode 109 of 2000 complete,  explore val = 0.01, episode reward = -117.34\n",
      "training episode 110 of 2000 complete,  explore val = 0.01, episode reward = -67.36\n",
      "training episode 111 of 2000 complete,  explore val = 0.01, episode reward = -100.98\n",
      "training episode 112 of 2000 complete,  explore val = 0.01, episode reward = -231.22\n",
      "training episode 113 of 2000 complete,  explore val = 0.01, episode reward = -134.42\n",
      "training episode 114 of 2000 complete,  explore val = 0.01, episode reward = -255.96\n",
      "training episode 115 of 2000 complete,  explore val = 0.01, episode reward = -250.72\n",
      "training episode 116 of 2000 complete,  explore val = 0.01, episode reward = -211.15\n",
      "training episode 117 of 2000 complete,  explore val = 0.01, episode reward = -259.14\n",
      "training episode 118 of 2000 complete,  explore val = 0.01, episode reward = -625.61\n",
      "training episode 119 of 2000 complete,  explore val = 0.01, episode reward = -180.17\n",
      "training episode 120 of 2000 complete,  explore val = 0.01, episode reward = -440.02\n",
      "training episode 121 of 2000 complete,  explore val = 0.01, episode reward = -385.65\n",
      "training episode 122 of 2000 complete,  explore val = 0.01, episode reward = -259.26\n",
      "training episode 123 of 2000 complete,  explore val = 0.01, episode reward = -378.84\n",
      "training episode 124 of 2000 complete,  explore val = 0.01, episode reward = -107.72\n",
      "training episode 125 of 2000 complete,  explore val = 0.01, episode reward = -338.08\n",
      "training episode 126 of 2000 complete,  explore val = 0.01, episode reward = -201.02\n",
      "training episode 127 of 2000 complete,  explore val = 0.01, episode reward = -637.13\n",
      "training episode 128 of 2000 complete,  explore val = 0.01, episode reward = -280.04\n",
      "training episode 129 of 2000 complete,  explore val = 0.01, episode reward = -400.5\n",
      "training episode 130 of 2000 complete,  explore val = 0.01, episode reward = -301.67\n",
      "training episode 131 of 2000 complete,  explore val = 0.01, episode reward = -107.01\n",
      "training episode 132 of 2000 complete,  explore val = 0.01, episode reward = -424.73\n",
      "training episode 133 of 2000 complete,  explore val = 0.01, episode reward = -426.93\n",
      "training episode 134 of 2000 complete,  explore val = 0.01, episode reward = -499.23\n",
      "training episode 135 of 2000 complete,  explore val = 0.01, episode reward = -229.25\n",
      "training episode 136 of 2000 complete,  explore val = 0.01, episode reward = -676.39\n",
      "training episode 137 of 2000 complete,  explore val = 0.01, episode reward = -284.93\n",
      "training episode 138 of 2000 complete,  explore val = 0.01, episode reward = -251.92\n",
      "training episode 139 of 2000 complete,  explore val = 0.01, episode reward = -264.7\n",
      "training episode 140 of 2000 complete,  explore val = 0.01, episode reward = -380.59\n",
      "training episode 141 of 2000 complete,  explore val = 0.01, episode reward = -360.89\n",
      "training episode 142 of 2000 complete,  explore val = 0.01, episode reward = -662.77\n",
      "training episode 143 of 2000 complete,  explore val = 0.01, episode reward = -185.74\n",
      "training episode 144 of 2000 complete,  explore val = 0.01, episode reward = -364.26\n",
      "training episode 145 of 2000 complete,  explore val = 0.01, episode reward = -257.71\n",
      "training episode 146 of 2000 complete,  explore val = 0.01, episode reward = -459.21\n",
      "training episode 147 of 2000 complete,  explore val = 0.01, episode reward = -167.68\n",
      "training episode 148 of 2000 complete,  explore val = 0.01, episode reward = -852.39\n",
      "training episode 149 of 2000 complete,  explore val = 0.01, episode reward = -233.59\n",
      "training episode 150 of 2000 complete,  explore val = 0.01, episode reward = -671.39\n",
      "training episode 151 of 2000 complete,  explore val = 0.01, episode reward = -599.03\n",
      "training episode 152 of 2000 complete,  explore val = 0.01, episode reward = -244.06\n",
      "training episode 153 of 2000 complete,  explore val = 0.01, episode reward = -374.53\n",
      "training episode 154 of 2000 complete,  explore val = 0.01, episode reward = -148.92\n",
      "training episode 155 of 2000 complete,  explore val = 0.01, episode reward = -470.84\n",
      "training episode 156 of 2000 complete,  explore val = 0.01, episode reward = -191.31\n",
      "training episode 157 of 2000 complete,  explore val = 0.01, episode reward = -370.31\n",
      "training episode 158 of 2000 complete,  explore val = 0.01, episode reward = -896.12\n",
      "training episode 159 of 2000 complete,  explore val = 0.01, episode reward = -348.33\n",
      "training episode 160 of 2000 complete,  explore val = 0.01, episode reward = -626.08\n",
      "training episode 161 of 2000 complete,  explore val = 0.01, episode reward = -370.57\n",
      "training episode 162 of 2000 complete,  explore val = 0.01, episode reward = -414.05\n",
      "training episode 163 of 2000 complete,  explore val = 0.01, episode reward = -418.17\n",
      "training episode 164 of 2000 complete,  explore val = 0.01, episode reward = -77.13\n",
      "training episode 165 of 2000 complete,  explore val = 0.01, episode reward = -187.73\n",
      "training episode 166 of 2000 complete,  explore val = 0.01, episode reward = -155.1\n",
      "training episode 167 of 2000 complete,  explore val = 0.01, episode reward = 91.12\n",
      "training episode 168 of 2000 complete,  explore val = 0.01, episode reward = -648.93\n",
      "training episode 169 of 2000 complete,  explore val = 0.01, episode reward = -175.88\n",
      "training episode 170 of 2000 complete,  explore val = 0.01, episode reward = -387.78\n",
      "training episode 171 of 2000 complete,  explore val = 0.01, episode reward = -398.49\n",
      "training episode 172 of 2000 complete,  explore val = 0.01, episode reward = -269.88\n",
      "training episode 173 of 2000 complete,  explore val = 0.01, episode reward = -309.1\n",
      "training episode 174 of 2000 complete,  explore val = 0.01, episode reward = -396.95\n",
      "training episode 175 of 2000 complete,  explore val = 0.01, episode reward = -90.84\n",
      "training episode 176 of 2000 complete,  explore val = 0.01, episode reward = -117.83\n",
      "training episode 177 of 2000 complete,  explore val = 0.01, episode reward = -224.89\n",
      "training episode 178 of 2000 complete,  explore val = 0.01, episode reward = -375.99\n",
      "training episode 179 of 2000 complete,  explore val = 0.01, episode reward = -57.29\n",
      "training episode 180 of 2000 complete,  explore val = 0.01, episode reward = -254.12\n",
      "training episode 181 of 2000 complete,  explore val = 0.01, episode reward = -402.93\n",
      "training episode 182 of 2000 complete,  explore val = 0.01, episode reward = -149.67\n",
      "training episode 183 of 2000 complete,  explore val = 0.01, episode reward = -281.6\n",
      "training episode 184 of 2000 complete,  explore val = 0.01, episode reward = -352.16\n",
      "training episode 185 of 2000 complete,  explore val = 0.01, episode reward = -355.67\n",
      "training episode 186 of 2000 complete,  explore val = 0.01, episode reward = -400.71\n",
      "training episode 187 of 2000 complete,  explore val = 0.01, episode reward = -423.98\n",
      "training episode 188 of 2000 complete,  explore val = 0.01, episode reward = -247.5\n",
      "training episode 189 of 2000 complete,  explore val = 0.01, episode reward = -381.76\n",
      "training episode 190 of 2000 complete,  explore val = 0.01, episode reward = -323.56\n",
      "training episode 191 of 2000 complete,  explore val = 0.01, episode reward = -362.32\n",
      "training episode 192 of 2000 complete,  explore val = 0.01, episode reward = -159.36\n",
      "training episode 193 of 2000 complete,  explore val = 0.01, episode reward = -255.06\n",
      "training episode 194 of 2000 complete,  explore val = 0.01, episode reward = -206.8\n",
      "training episode 195 of 2000 complete,  explore val = 0.01, episode reward = -330.62\n",
      "training episode 196 of 2000 complete,  explore val = 0.01, episode reward = -462.84\n",
      "training episode 197 of 2000 complete,  explore val = 0.01, episode reward = -313.79\n",
      "training episode 198 of 2000 complete,  explore val = 0.01, episode reward = -478.31\n",
      "training episode 199 of 2000 complete,  explore val = 0.01, episode reward = -347.75\n",
      "training episode 200 of 2000 complete,  explore val = 0.01, episode reward = -276.37\n",
      "training episode 201 of 2000 complete,  explore val = 0.01, episode reward = -266.78\n",
      "training episode 202 of 2000 complete,  explore val = 0.01, episode reward = -126.83\n",
      "training episode 203 of 2000 complete,  explore val = 0.01, episode reward = -180.41\n",
      "training episode 204 of 2000 complete,  explore val = 0.01, episode reward = -196.11\n",
      "training episode 205 of 2000 complete,  explore val = 0.01, episode reward = -281.98\n",
      "training episode 206 of 2000 complete,  explore val = 0.01, episode reward = -283.04\n",
      "training episode 207 of 2000 complete,  explore val = 0.01, episode reward = -209.62\n",
      "training episode 208 of 2000 complete,  explore val = 0.01, episode reward = -231.28\n",
      "training episode 209 of 2000 complete,  explore val = 0.01, episode reward = -30.49\n",
      "training episode 210 of 2000 complete,  explore val = 0.01, episode reward = -372.99\n",
      "training episode 211 of 2000 complete,  explore val = 0.01, episode reward = -398.83\n",
      "training episode 212 of 2000 complete,  explore val = 0.01, episode reward = -137.87\n",
      "training episode 213 of 2000 complete,  explore val = 0.01, episode reward = -290.07\n",
      "training episode 214 of 2000 complete,  explore val = 0.01, episode reward = -116.15\n",
      "training episode 215 of 2000 complete,  explore val = 0.01, episode reward = -229.95\n",
      "training episode 216 of 2000 complete,  explore val = 0.01, episode reward = -150.68\n",
      "training episode 217 of 2000 complete,  explore val = 0.01, episode reward = -104.48\n",
      "training episode 218 of 2000 complete,  explore val = 0.01, episode reward = -607.11\n",
      "training episode 219 of 2000 complete,  explore val = 0.01, episode reward = -420.2\n",
      "training episode 220 of 2000 complete,  explore val = 0.01, episode reward = -402.57\n",
      "training episode 221 of 2000 complete,  explore val = 0.01, episode reward = -316.28\n",
      "training episode 222 of 2000 complete,  explore val = 0.01, episode reward = -93.13\n",
      "training episode 223 of 2000 complete,  explore val = 0.01, episode reward = -285.31\n",
      "training episode 224 of 2000 complete,  explore val = 0.01, episode reward = -418.49\n",
      "training episode 225 of 2000 complete,  explore val = 0.01, episode reward = -551.39\n",
      "training episode 226 of 2000 complete,  explore val = 0.01, episode reward = -483.31\n",
      "training episode 227 of 2000 complete,  explore val = 0.01, episode reward = -261.02\n",
      "training episode 228 of 2000 complete,  explore val = 0.01, episode reward = -330.96\n",
      "training episode 229 of 2000 complete,  explore val = 0.01, episode reward = -361.0\n",
      "training episode 230 of 2000 complete,  explore val = 0.01, episode reward = -557.24\n",
      "training episode 231 of 2000 complete,  explore val = 0.01, episode reward = -256.39\n",
      "training episode 232 of 2000 complete,  explore val = 0.01, episode reward = -239.62\n",
      "training episode 233 of 2000 complete,  explore val = 0.01, episode reward = -120.96\n",
      "training episode 234 of 2000 complete,  explore val = 0.01, episode reward = -316.33\n",
      "training episode 235 of 2000 complete,  explore val = 0.01, episode reward = -252.53\n",
      "training episode 236 of 2000 complete,  explore val = 0.01, episode reward = -666.05\n",
      "training episode 237 of 2000 complete,  explore val = 0.01, episode reward = -297.13\n",
      "training episode 238 of 2000 complete,  explore val = 0.01, episode reward = -348.55\n",
      "training episode 239 of 2000 complete,  explore val = 0.01, episode reward = -291.67\n",
      "training episode 240 of 2000 complete,  explore val = 0.01, episode reward = -408.82\n",
      "training episode 241 of 2000 complete,  explore val = 0.01, episode reward = -530.56\n",
      "training episode 242 of 2000 complete,  explore val = 0.01, episode reward = -237.07\n",
      "training episode 243 of 2000 complete,  explore val = 0.01, episode reward = -441.94\n",
      "training episode 244 of 2000 complete,  explore val = 0.01, episode reward = -411.77\n",
      "training episode 245 of 2000 complete,  explore val = 0.01, episode reward = -309.02\n",
      "training episode 246 of 2000 complete,  explore val = 0.01, episode reward = -183.9\n",
      "training episode 247 of 2000 complete,  explore val = 0.01, episode reward = -137.37\n",
      "training episode 248 of 2000 complete,  explore val = 0.01, episode reward = -641.49\n",
      "training episode 249 of 2000 complete,  explore val = 0.01, episode reward = -460.06\n",
      "training episode 250 of 2000 complete,  explore val = 0.01, episode reward = -332.91\n",
      "training episode 251 of 2000 complete,  explore val = 0.01, episode reward = -70.0\n",
      "training episode 252 of 2000 complete,  explore val = 0.01, episode reward = -78.04\n",
      "training episode 253 of 2000 complete,  explore val = 0.01, episode reward = -243.46\n",
      "training episode 254 of 2000 complete,  explore val = 0.01, episode reward = -567.14\n",
      "training episode 255 of 2000 complete,  explore val = 0.01, episode reward = -1090.18\n",
      "training episode 256 of 2000 complete,  explore val = 0.01, episode reward = -262.47\n",
      "training episode 257 of 2000 complete,  explore val = 0.01, episode reward = -514.33\n",
      "training episode 258 of 2000 complete,  explore val = 0.01, episode reward = -276.88\n",
      "training episode 259 of 2000 complete,  explore val = 0.01, episode reward = -459.4\n",
      "training episode 260 of 2000 complete,  explore val = 0.01, episode reward = -280.77\n",
      "training episode 261 of 2000 complete,  explore val = 0.01, episode reward = -76.33\n",
      "training episode 262 of 2000 complete,  explore val = 0.01, episode reward = -130.14\n",
      "training episode 263 of 2000 complete,  explore val = 0.01, episode reward = -517.3\n",
      "training episode 264 of 2000 complete,  explore val = 0.01, episode reward = -314.79\n",
      "training episode 265 of 2000 complete,  explore val = 0.01, episode reward = -261.93\n",
      "training episode 266 of 2000 complete,  explore val = 0.01, episode reward = -62.43\n",
      "training episode 267 of 2000 complete,  explore val = 0.01, episode reward = -158.7\n",
      "training episode 268 of 2000 complete,  explore val = 0.01, episode reward = -451.75\n",
      "training episode 269 of 2000 complete,  explore val = 0.01, episode reward = -144.94\n",
      "training episode 270 of 2000 complete,  explore val = 0.01, episode reward = -346.74\n",
      "training episode 271 of 2000 complete,  explore val = 0.01, episode reward = -195.37\n",
      "training episode 272 of 2000 complete,  explore val = 0.01, episode reward = -324.58\n",
      "training episode 273 of 2000 complete,  explore val = 0.01, episode reward = -374.02\n",
      "training episode 274 of 2000 complete,  explore val = 0.01, episode reward = -287.82\n",
      "training episode 275 of 2000 complete,  explore val = 0.01, episode reward = -480.84\n",
      "training episode 276 of 2000 complete,  explore val = 0.01, episode reward = -607.15\n",
      "training episode 277 of 2000 complete,  explore val = 0.01, episode reward = -611.8\n",
      "training episode 278 of 2000 complete,  explore val = 0.01, episode reward = -95.78\n",
      "training episode 279 of 2000 complete,  explore val = 0.01, episode reward = -128.09\n",
      "training episode 280 of 2000 complete,  explore val = 0.01, episode reward = -226.68\n",
      "training episode 281 of 2000 complete,  explore val = 0.01, episode reward = -129.17\n",
      "training episode 282 of 2000 complete,  explore val = 0.01, episode reward = -193.0\n",
      "training episode 283 of 2000 complete,  explore val = 0.01, episode reward = -143.94\n",
      "training episode 284 of 2000 complete,  explore val = 0.01, episode reward = -352.3\n",
      "training episode 285 of 2000 complete,  explore val = 0.01, episode reward = -503.76\n",
      "training episode 286 of 2000 complete,  explore val = 0.01, episode reward = -404.24\n",
      "training episode 287 of 2000 complete,  explore val = 0.01, episode reward = -84.43\n",
      "training episode 288 of 2000 complete,  explore val = 0.01, episode reward = -811.63\n",
      "training episode 289 of 2000 complete,  explore val = 0.01, episode reward = -485.31\n",
      "training episode 290 of 2000 complete,  explore val = 0.01, episode reward = -569.1\n",
      "training episode 291 of 2000 complete,  explore val = 0.01, episode reward = -2078.05\n",
      "training episode 292 of 2000 complete,  explore val = 0.01, episode reward = -129.51\n",
      "training episode 293 of 2000 complete,  explore val = 0.01, episode reward = -247.36\n",
      "training episode 294 of 2000 complete,  explore val = 0.01, episode reward = -161.85\n",
      "training episode 295 of 2000 complete,  explore val = 0.01, episode reward = -342.16\n",
      "training episode 296 of 2000 complete,  explore val = 0.01, episode reward = -344.43\n",
      "training episode 297 of 2000 complete,  explore val = 0.01, episode reward = -323.89\n",
      "training episode 298 of 2000 complete,  explore val = 0.01, episode reward = -388.38\n",
      "training episode 299 of 2000 complete,  explore val = 0.01, episode reward = -579.13\n",
      "training episode 300 of 2000 complete,  explore val = 0.01, episode reward = -482.58\n",
      "training episode 301 of 2000 complete,  explore val = 0.01, episode reward = -218.08\n",
      "training episode 302 of 2000 complete,  explore val = 0.01, episode reward = -313.55\n",
      "training episode 303 of 2000 complete,  explore val = 0.01, episode reward = -184.16\n",
      "training episode 304 of 2000 complete,  explore val = 0.01, episode reward = -458.99\n",
      "training episode 305 of 2000 complete,  explore val = 0.01, episode reward = -195.84\n",
      "training episode 306 of 2000 complete,  explore val = 0.01, episode reward = -628.43\n",
      "training episode 307 of 2000 complete,  explore val = 0.01, episode reward = -634.91\n",
      "training episode 308 of 2000 complete,  explore val = 0.01, episode reward = -167.77\n",
      "training episode 309 of 2000 complete,  explore val = 0.01, episode reward = -170.92\n",
      "training episode 310 of 2000 complete,  explore val = 0.01, episode reward = -342.22\n",
      "training episode 311 of 2000 complete,  explore val = 0.01, episode reward = -141.95\n",
      "training episode 312 of 2000 complete,  explore val = 0.01, episode reward = -377.83\n",
      "training episode 313 of 2000 complete,  explore val = 0.01, episode reward = -271.73\n",
      "training episode 314 of 2000 complete,  explore val = 0.01, episode reward = -203.37\n",
      "training episode 315 of 2000 complete,  explore val = 0.01, episode reward = -225.21\n",
      "training episode 316 of 2000 complete,  explore val = 0.01, episode reward = -207.45\n",
      "training episode 317 of 2000 complete,  explore val = 0.01, episode reward = -54.85\n",
      "training episode 318 of 2000 complete,  explore val = 0.01, episode reward = -146.26\n",
      "training episode 319 of 2000 complete,  explore val = 0.01, episode reward = -404.53\n",
      "training episode 320 of 2000 complete,  explore val = 0.01, episode reward = -381.58\n",
      "training episode 321 of 2000 complete,  explore val = 0.01, episode reward = -142.28\n",
      "training episode 322 of 2000 complete,  explore val = 0.01, episode reward = -277.49\n",
      "training episode 323 of 2000 complete,  explore val = 0.01, episode reward = -86.87\n",
      "training episode 324 of 2000 complete,  explore val = 0.01, episode reward = -350.98\n",
      "training episode 325 of 2000 complete,  explore val = 0.01, episode reward = -155.73\n",
      "training episode 326 of 2000 complete,  explore val = 0.01, episode reward = -146.16\n",
      "training episode 327 of 2000 complete,  explore val = 0.01, episode reward = -136.19\n",
      "training episode 328 of 2000 complete,  explore val = 0.01, episode reward = -126.42\n",
      "training episode 329 of 2000 complete,  explore val = 0.01, episode reward = -182.12\n",
      "training episode 330 of 2000 complete,  explore val = 0.01, episode reward = -170.1\n",
      "training episode 331 of 2000 complete,  explore val = 0.01, episode reward = -165.5\n",
      "training episode 332 of 2000 complete,  explore val = 0.01, episode reward = -140.23\n",
      "training episode 333 of 2000 complete,  explore val = 0.01, episode reward = -215.47\n",
      "training episode 334 of 2000 complete,  explore val = 0.01, episode reward = -403.11\n",
      "training episode 335 of 2000 complete,  explore val = 0.01, episode reward = -596.05\n",
      "training episode 336 of 2000 complete,  explore val = 0.01, episode reward = -407.07\n",
      "training episode 337 of 2000 complete,  explore val = 0.01, episode reward = -367.83\n",
      "training episode 338 of 2000 complete,  explore val = 0.01, episode reward = -241.51\n",
      "training episode 339 of 2000 complete,  explore val = 0.01, episode reward = -183.7\n",
      "training episode 340 of 2000 complete,  explore val = 0.01, episode reward = -286.37\n",
      "training episode 341 of 2000 complete,  explore val = 0.01, episode reward = -156.8\n",
      "training episode 342 of 2000 complete,  explore val = 0.01, episode reward = -167.5\n",
      "training episode 343 of 2000 complete,  explore val = 0.01, episode reward = -316.32\n",
      "training episode 344 of 2000 complete,  explore val = 0.01, episode reward = -166.13\n",
      "training episode 345 of 2000 complete,  explore val = 0.01, episode reward = -144.97\n",
      "training episode 346 of 2000 complete,  explore val = 0.01, episode reward = -268.79\n",
      "training episode 347 of 2000 complete,  explore val = 0.01, episode reward = -240.83\n",
      "training episode 348 of 2000 complete,  explore val = 0.01, episode reward = -114.13\n",
      "training episode 349 of 2000 complete,  explore val = 0.01, episode reward = -114.6\n",
      "training episode 350 of 2000 complete,  explore val = 0.01, episode reward = -758.39\n",
      "training episode 351 of 2000 complete,  explore val = 0.01, episode reward = -876.2\n",
      "training episode 352 of 2000 complete,  explore val = 0.01, episode reward = -205.07\n",
      "training episode 353 of 2000 complete,  explore val = 0.01, episode reward = -362.14\n",
      "training episode 354 of 2000 complete,  explore val = 0.01, episode reward = -297.06\n",
      "training episode 355 of 2000 complete,  explore val = 0.01, episode reward = -446.12\n",
      "training episode 356 of 2000 complete,  explore val = 0.01, episode reward = -411.17\n",
      "training episode 357 of 2000 complete,  explore val = 0.01, episode reward = -276.45\n",
      "training episode 358 of 2000 complete,  explore val = 0.01, episode reward = -530.36\n",
      "training episode 359 of 2000 complete,  explore val = 0.01, episode reward = -366.34\n",
      "training episode 360 of 2000 complete,  explore val = 0.01, episode reward = -371.88\n",
      "training episode 361 of 2000 complete,  explore val = 0.01, episode reward = -1032.5\n",
      "training episode 362 of 2000 complete,  explore val = 0.01, episode reward = -357.65\n",
      "training episode 363 of 2000 complete,  explore val = 0.01, episode reward = -360.8\n",
      "training episode 364 of 2000 complete,  explore val = 0.01, episode reward = -201.44\n",
      "training episode 365 of 2000 complete,  explore val = 0.01, episode reward = -242.74\n",
      "training episode 366 of 2000 complete,  explore val = 0.01, episode reward = -115.72\n",
      "training episode 367 of 2000 complete,  explore val = 0.01, episode reward = -349.89\n",
      "training episode 368 of 2000 complete,  explore val = 0.01, episode reward = -76.79\n",
      "training episode 369 of 2000 complete,  explore val = 0.01, episode reward = -125.42\n",
      "training episode 370 of 2000 complete,  explore val = 0.01, episode reward = -469.69\n",
      "training episode 371 of 2000 complete,  explore val = 0.01, episode reward = -423.47\n",
      "training episode 372 of 2000 complete,  explore val = 0.01, episode reward = -297.08\n",
      "training episode 373 of 2000 complete,  explore val = 0.01, episode reward = -355.31\n",
      "training episode 374 of 2000 complete,  explore val = 0.01, episode reward = -192.53\n",
      "training episode 375 of 2000 complete,  explore val = 0.01, episode reward = -488.89\n",
      "training episode 376 of 2000 complete,  explore val = 0.01, episode reward = -275.27\n",
      "training episode 377 of 2000 complete,  explore val = 0.01, episode reward = -271.44\n",
      "training episode 378 of 2000 complete,  explore val = 0.01, episode reward = -161.55\n",
      "training episode 379 of 2000 complete,  explore val = 0.01, episode reward = -427.48\n",
      "training episode 380 of 2000 complete,  explore val = 0.01, episode reward = -362.93\n",
      "training episode 532 of 2000 complete,  explore val = 0.01, episode reward = -1098.83\n",
      "training episode 533 of 2000 complete,  explore val = 0.01, episode reward = -447.7\n",
      "training episode 534 of 2000 complete,  explore val = 0.01, episode reward = -192.37\n",
      "training episode 535 of 2000 complete,  explore val = 0.01, episode reward = -446.19\n",
      "training episode 536 of 2000 complete,  explore val = 0.01, episode reward = -324.16\n",
      "training episode 537 of 2000 complete,  explore val = 0.01, episode reward = -213.68\n",
      "training episode 538 of 2000 complete,  explore val = 0.01, episode reward = -125.98\n",
      "training episode 539 of 2000 complete,  explore val = 0.01, episode reward = -335.22\n",
      "training episode 540 of 2000 complete,  explore val = 0.01, episode reward = -274.22\n",
      "training episode 541 of 2000 complete,  explore val = 0.01, episode reward = -358.43\n",
      "training episode 542 of 2000 complete,  explore val = 0.01, episode reward = -309.88\n",
      "training episode 543 of 2000 complete,  explore val = 0.01, episode reward = -850.23\n",
      "training episode 544 of 2000 complete,  explore val = 0.01, episode reward = -1080.22\n",
      "training episode 545 of 2000 complete,  explore val = 0.01, episode reward = -434.31\n",
      "training episode 546 of 2000 complete,  explore val = 0.01, episode reward = -116.26\n",
      "training episode 547 of 2000 complete,  explore val = 0.01, episode reward = -317.98\n"
     ]
    }
   ],
   "source": [
    "demo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot total episode reward history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_reward_history(logname,**kwargs):\n",
    "    # load in total episode reward history\n",
    "    data = np.loadtxt(logname)\n",
    "    \n",
    "    # create figure\n",
    "    fig = plt.figure(figsize = (12,5))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    # plot total reward history\n",
    "    start = 0\n",
    "    if 'start' in kwargs:\n",
    "        start = kwargs['start']\n",
    "    ax.plot(data[start:])\n",
    "    ax.set_xlabel('episode')\n",
    "    ax.set_ylabel('total reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "plot_reward_history(reward_logname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# produce animation of validation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load in weights\n",
    "infile = demo.weight_name\n",
    "with open(infile, 'rb') as in_strm:\n",
    "    datastruct = pickle.load(in_strm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(datastruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
